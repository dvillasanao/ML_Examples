---
title: "Locally Estimated Scatterplot Smoothing (LOESS)"
subtitle: "Apuntes y anotaciones personales"
author: "Diana Villasana Ocampo"
knit: (function(inputFile, encoding) {
       rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../Output/Regression")
  })
output:
   html_document:
      code_folding: hide
      highlight: tango
      theme: flatly
      toc: true
      toc_depth: 3
      toc_float:
        collapsed: yes
      css: ../../R/style.css
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, 
                      cache.lazy = FALSE, class.source = "fold-show")
knitr::opts_knit$set(root.dir = here::here())
setwd(here::here())
```

```{r, echo=FALSE}
rm(list = ls())
```

```{r, echo = FALSE, results=FALSE}
# Se descargan las fuentes de la google fonts
require(showtext)
library(extrafont)
# activar showtext
windowsFonts()
```

```{r, echo = FALSE}
# 1. Cargar librerías necesarias
library(tidyverse)
require(knitr)
library(caret)     # Para dividir datos y evaluación
library(broom)     # Para tidy modelos
library(Metrics)   # Para métricas como RMSE, MAE
require(tibble)
require(gt)
```



```{r echo=FALSE, fig.show="hold", out.width="48%", eval = FALSE}
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS.png"))
knitr::include_graphics(paste0(here::here(), "/img/Regression/LOESS_1.png"))
```

## LOESS (Locally Estimated Scatterplot Smoothing)

LOESS, o *Locally Estimated Scatterplot Smoothing* (Suavizado de Diagramas de Dispersión Estimado Localmente), es un método de regresión no paramétrico que se utiliza para **ajustar una curva suave a través de un diagrama de dispersión de datos**. A diferencia de los métodos de regresión paramétricos (como la regresión lineal o polinomial simple), LOESS no asume una forma funcional predefinida para la relación entre las variables. En su lugar, construye la curva ajustada calculando múltiples regresiones locales.  

```{r, echo = FALSE}
criterios <- c(
  "Tipo de modelo",
  "Variable respuesta",
  "Variables predictoras",
  "Relación entre variables",
  "Normalidad de residuos",
  "Independencia de errores",
  "Homoscedasticidad",
  "Sensible a outliers",
  "Multicolinealidad entre predictores",
  "Interpretabilidad",
  "Velocidad y eficiencia",
  "Validación cruzada",
  "No funciona bien si..."
)
aplica <- c(
  "✅ Supervisado",
  "✅ Continua",
  "✅ Numéricas (usualmente 1 o 2 predictores)",
  "✅ No lineal y suave",
  "❌ No necesaria",
  "✅ Deseable",
  "✅ Deseable",
  "⚠️ Muy sensible",
  "❌ No aplica (pocos predictores)",
  "✅ Muy interpretable gráficamente",
  "⚠️ Lento en grandes volúmenes de datos",
  "✅ Puede usarse para elegir 'span'",
  "❌ Datos grandes o con ruido fuerte"
)
detalles <- c(
  "Modelo no paramétrico local",
  "Regresión para valores continuos",
  "Generalmente 1 o 2 variables numéricas",
  "Ajuste por vecindad, suaviza la curva",
  "No asume distribución específica",
  "Supuesto deseable si hay dependencias temporales",
  "Ideal si la varianza no cambia mucho localmente",
  "Altamente afectado por outliers locales",
  "No es una técnica multivariable compleja",
  "La curva ajustada se interpreta visualmente",
  "Computacionalmente costoso con datos grandes",
  "Ayuda a seleccionar el mejor 'span'",
  "Poco eficaz en alta dimensión o datos muy dispersos"
)

tabla_loess <- data.frame(Criterio = criterios, Aplica = aplica, Detalles = detalles)

require(gt) 

tabla_loess %>%
 gt() %>%
  tab_header(title = "Guía rápida para elegir LOESS",
             subtitle = "Locally Estimated Scatterplot Smoothing (LOESS)") %>%
   tab_footnote(footnote = "Fuente: Elaboración propia") %>%
     tab_options(heading.title.font.size = 14,
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Detalles") ~ px(500),
                   everything() ~ px(200)) %>%
         as_raw_html()
```

</>

El principio central de LOESS es la **suavización local ponderada**. Para estimar el valor suavizado en un punto específico, el algoritmo:
  
  1.  Identifica un subconjunto de puntos de datos cercanos a ese punto.
2.  Asigna pesos a esos puntos cercanos, dando más peso a los puntos más próximos.
3.  Ajusta un polinomio de bajo grado (típicamente lineal o cuadrático) a esos puntos ponderados.
4.  El valor estimado para el punto de interés es el valor predicho por este polinomio local.

Este proceso se repite para múltiples puntos a lo largo del rango de la variable predictora para construir la curva suave completa. El parámetro más importante en LOESS es el **`span` (o ancho de banda)**, que controla la proporción de puntos utilizados en cada ajuste local y, por lo tanto, la suavidad de la curva resultante. Un `span` más pequeño resulta en una curva más "ondulada" que se ajusta más a los datos locales, mientras que un `span` más grande produce una curva más suave y generalizada.

## ¿Cuándo usar LOESS?

LOESS es particularmente útil en las siguientes situaciones:
  
  1.  **Análisis Exploratorio de Datos (EDA):** Es una excelente herramienta para visualizar las relaciones subyacentes entre dos variables (o una variable de respuesta y un predictor) sin imponer una forma funcional. Permite identificar tendencias no lineales y patrones complejos que podrían pasarse por alto con la regresión lineal simple.
2.  **Relaciones No Lineales:** Cuando sospechas o sabes que la relación entre tus variables no es lineal, pero no tienes una base teórica para especificar una función no lineal particular (exponencial, logarítmica, etc.). LOESS se adapta a la forma de los datos.
3.  **Identificación de Patrones Locales:** Si crees que la relación entre las variables puede cambiar a lo largo del rango de la variable predictora. LOESS puede capturar estos cambios locales.
4.  **Detección de Valores Atípicos:** La versión robusta de LOESS (cuando se usa `family = "symmetric"`) puede ser útil para identificar valores atípicos, ya que les da menos peso en el ajuste.
5.  **Suavizado de Series Temporales (no estacionales):** Aunque existen métodos específicos para series temporales, LOESS puede usarse para suavizar la tendencia en datos de series temporales que no tienen un componente estacional pronunciado.
6.  **Interpolación:** Para estimar valores de la variable de respuesta para valores de la variable predictora que se encuentran dentro del rango de los datos observados.

## Ventajas de LOESS

1.  **Flexibilidad y Adaptabilidad:** Es su mayor fortaleza. No requiere que el usuario especifique la forma funcional de la relación entre las variables. Se adapta a formas no lineales arbitrarias.
2.  **No Paramétrico:** Al no hacer suposiciones fuertes sobre la distribución de los datos o la forma de la relación, es muy robusto frente a violaciones de las suposiciones de los modelos paramétricos.
3.  **Captura Tendencias Locales:** Puede identificar cambios en la relación entre las variables a lo largo del rango del predictor.
4.  **Visualización Intuitiva:** La curva LOESS es fácil de interpretar visualmente, proporcionando una representación clara de la tendencia general de los datos.
5.  **Robustez (opcional):** La opción de ajuste robusto (`family = "symmetric"`) lo hace menos sensible a la influencia de los valores atípicos en el conjunto de datos.

## Desventajas de LOESS

1.  **Costo Computacional:** Puede ser computacionalmente intensivo, especialmente con grandes conjuntos de datos, ya que requiere múltiples ajustes de regresión locales para cada punto.
2.  **Dificultad con Múltiples Predictores:** Aunque teóricamente puede manejar múltiples predictores, su rendimiento y facilidad de interpretación disminuyen rápidamente a medida que aumenta el número de predictores (lo que se conoce como la "maldición de la dimensionalidad"). Es más adecuado para uno o dos predictores.
3.  **Falta de Modelo Explícito:** Al no tener una ecuación matemática subyacente, no proporciona un modelo paramétrico que pueda ser interpretado en términos de coeficientes o efectos específicos de los predictores. Esto puede limitar la inferencia estadística o la generalización más allá de los datos observados.
4.  **Parámetro `span`:** La selección del `span` óptimo puede ser subjetiva y a menudo requiere experimentación visual o métodos de validación cruzada, lo que añade complejidad.
5.  **Extrapolación Pobre:** LOESS es muy deficiente para la extrapolación (predicciones fuera del rango de los datos observados). Como el ajuste es local, no hay información en la periferia de los datos para realizar un ajuste fiable en puntos lejanos.
6.  **Dependencia de la Densidad de Datos:** Funciona mejor cuando los datos están distribuidos de manera relativamente uniforme. En áreas con pocos puntos de datos, el ajuste puede ser inestable o menos fiable.



## Algoritmo de LOESS (Locally Estimated Scatterplot Smoothing)  

El algoritmo `LOESS` (**Locally Estimated Scatterplot Smoothing**) es un método de regresión no paramétrico que se utiliza para ajustar una curva suave a través de un diagrama de dispersión. Su principal ventaja es que no requiere que se especifique una función matemática previa para la relación entre las variables, lo que lo hace muy flexible para capturar patrones no lineales y complejos en los datos.   

**1. Parámetros de entrada:**
  
  * **Datos:** Un conjunto de puntos $(x_i, y_i)$, donde $x_i$ es la variable predictora e $y_i$ es la variable de respuesta.
* **Span (parámetro de suavizado o ancho de banda):** Es un valor entre 0 y 1 que determina la proporción de puntos del conjunto de datos que se utilizarán para cada ajuste local.
* Un `span` pequeño (cerca de 0) utiliza menos puntos y produce una curva que se ajusta más a las fluctuaciones locales (menos suavizada, potencialmente más propensa al sobreajuste).
* Un `span` grande (cerca de 1) utiliza más puntos y produce una curva más suave que generaliza más (más suavizada, potencialmente más propensa al subajuste).
* **Grado del polinomio local:** Generalmente se utiliza un polinomio de grado 1 (lineal) o 2 (cuadrático) para los ajustes locales.
* **Función de ponderación:** La función más común es la función de ponderación tricúbica.

**2. Proceso para cada punto de estimación ($x_0$):**
  
  El objetivo de LOESS es estimar un valor suavizado $\hat{y}_0$ para cada punto $x_0$ (o para una rejilla de puntos $x_0$ para visualizar la curva). Para cada $x_0$:
  
  a. **Seleccionar los vecinos más cercanos:**
  * Identifica los $k$ puntos más cercanos a $x_0$, donde $k$ se determina por el `span` ($k = \text{round}(\text{span} \times N)$, siendo $N$ el número total de puntos en el conjunto de datos).
* Estos puntos forman el "subconjunto local" para el ajuste en $x_0$.

b. **Calcular las ponderaciones:**
  * Para cada punto $(x_i, y_i)$ en el subconjunto local, calcula su distancia $d_i = |x_0 - x_i|$.
* Encuentra la distancia máxima $D_{max}$ entre $x_0$ y cualquier punto en el subconjunto local.
* Normaliza las distancias: $d_i^* = \frac{d_i}{D_{max}}$.
* Aplica la función de ponderación tricúbica para obtener la ponderación $w_i$ para cada punto $i$:

 Esta función determina el peso de cada punto vecino:
  
  $$w_j = \left(1 - \left( \frac{|x_j - x_i|}{d_{\text{max}}} \right)^3 \right)^3 \quad \text{si } |x_j - x_i| < d_{\text{max}}$$

  $$w_i = (1 - (d_i^*)^3)^3 \quad \text{si } d_i^* < 1$$      
  
  $$w_i = 0 \quad \text{si } d_i^* \ge 1$$
  
  Esta función asigna mayores pesos a los puntos más cercanos a $x_0$ y pesos decrecientes a los puntos más alejados, hasta llegar a cero para los puntos fuera del rango definido por el `span`.

c. **Realizar una regresión por mínimos cuadrados ponderados:**
  * Utiliza los puntos del subconjunto local y sus ponderaciones calculadas ($w_i$) para ajustar un polinomio (de grado 1 o 2) por mínimos cuadrados ponderados.  
  
* Es decir, se minimiza la suma de los errores cuadrados, donde cada error se multiplica por su peso correspondiente:
  $$\sum_{i \in \text{subconjunto local}} w_i (y_i - \hat{y}_i)^2$$
  
  Donde $\hat{y}_i$ es el valor predicho por el polinomio local para $x_i$.

d. **Estimar el valor suavizado:**
  * Una vez que el polinomio local ha sido ajustado, se utiliza para predecir el valor $\hat{y}_0$ evaluando el polinomio en $x_0$.

**3. Repetir para todos los puntos:**
  
  Estos pasos se repiten para cada punto $x_0$ para el cual se desea obtener un valor suavizado. Si se desea una curva suave para la visualización, se suelen seleccionar una serie de puntos equiespaciados a lo largo del rango de $x$.

**Consideraciones adicionales (robustez):**
  
  LOESS también puede incorporar un paso de robustificación para manejar valores atípicos (outliers). Esto implica una iteración adicional:
  
  1.  Después de la primera pasada del LOESS, se calculan los residuos para cada punto.
2.  Se asignan nuevos pesos a los puntos basándose en la magnitud de sus residuos (los puntos con residuos grandes reciben pesos más pequeños).
3.  El algoritmo LOESS se ejecuta nuevamente utilizando estos nuevos pesos, lo que reduce la influencia de los valores atípicos en el ajuste final.


## Valores predictivos   

Una vez que el modelo LOESS ha sido "entrenado" o ajustado a tus datos existentes, el predictor se usa de una manera muy intuitiva para obtener valores suavizados o predicciones para nuevos puntos de la variable predictora.

**1. El Proceso de Predicción en LOESS**

A diferencia de los modelos paramétricos (como la regresión lineal) donde tienes una ecuación de forma fija para la predicción, LOESS es un método **local**. Esto significa que para predecir un valor de $y$ para un nuevo $x$ (digamos $x_{nuevo}$), el algoritmo realiza un ajuste local **en el momento de la predicción**.

Los pasos son esencialmente los mismos que los del ajuste inicial, pero enfocados en el punto $x_{nuevo}$:

a. **Identificar el punto de interés ($x_{nuevo}$):** Este es el valor de la variable predictora para el cual quieres obtener una estimación suavizada o una predicción.

b. **Seleccionar los vecinos más cercanos:**
   * El algoritmo busca los $k$ puntos más cercanos a $x_{nuevo}$ en el conjunto de datos **original** (los datos que se usaron para "entrenar" el LOESS). El valor de $k$ sigue siendo determinado por el `span` que se definió al construir el modelo.
   * Estos puntos son los que influirán en la predicción para $x_{nuevo}$.

c. **Calcular las ponderaciones:**
   * Para cada uno de esos $k$ puntos vecinos, se calcula una ponderación utilizando la misma función de ponderación tricúbica que se usó en el ajuste inicial.
   * Las ponderaciones dependen de la distancia de cada vecino a $x_{nuevo}$. Los puntos más cercanos a $x_{nuevo}$ reciben ponderaciones más altas.

d. **Realizar una regresión por mínimos cuadrados ponderados local:**
   * Con los $k$ puntos vecinos y sus ponderaciones, se ajusta un polinomio (del mismo grado que se usó en el ajuste inicial, típicamente lineal o cuadrático) utilizando mínimos cuadrados ponderados.
   * Este polinomio es *específico* para la predicción de $x_{nuevo}$.

e. **Evaluar el polinomio para obtener la predicción ($\hat{y}_{nuevo}$):**
   * Finalmente, se evalúa el polinomio local recién ajustado en $x_{nuevo}$ para obtener el valor predicho $\hat{y}_{nuevo}$.

**2. Diferencia Clave con Modelos Paramétricos**

* **Sin Ecuación Única:** En LOESS, no hay una única ecuación global que puedas "escribir" y usar para enchufar cualquier $x_{nuevo}$ y obtener una $\hat{y}$. Cada predicción para un $x_{nuevo}$ implica un nuevo ajuste local.
* **Dependencia del Conjunto de Datos Original:** El predictor LOESS siempre necesita acceso al conjunto de datos original porque necesita encontrar los vecinos más cercanos para cada nueva predicción. No "aprende" un conjunto fijo de coeficientes o parámetros.

**3. ¿Cuándo se usa el predictor LOESS?**

* **Suavizado de Datos Existentes:** La aplicación más común del "predictor" LOESS es para generar la curva suave en el rango de los datos ya observados. Para cada $x_i$ en tu conjunto de datos, o para una rejilla de $x$ a lo largo de tu rango de datos, el LOESS calcula un $\hat{y}_i$ suavizado. Esto te permite visualizar la tendencia subyacente.
* **Imputación de Valores Faltantes (con precaución):** Si tienes un $x$ para el cual falta su $y$, podrías usar LOESS para estimar ese $y$ basándose en los $x$ y $y$ circundantes.
* **Predicción para Nuevos Puntos (extrapolación limitada):** Aunque LOESS es excelente para interpolar (predecir dentro del rango de tus datos observados), es **extremadamente cauteloso y generalmente no recomendado para la extrapolación** (predecir fuera del rango de tus datos). Si $x_{nuevo}$ está muy lejos de tus datos observados, no habrá vecinos cercanos en el conjunto de datos original para realizar un ajuste local significativo, y las predicciones serán muy poco fiables.


## Parámetros   

```{r, eval = FALSE}
loess(formula, data, weights, subset, na.action, model = FALSE,
      span = 0.75, enp.target, degree = 2,
      parametric = FALSE, drop.square = FALSE, normalize = TRUE,
      family = c("gaussian", "symmetric"),
      method = c("loess", "model.frame"),
      control = loess.control(…), …)
```


La función `loess()` en R se utiliza para ajustar modelos de regresión local (Local Polynomial Regression). Acepta varios parámetros que permiten controlar el comportamiento del ajuste. A continuación, te detallo cada uno de ellos:

</b>

**Parámetros principales de `loess()`**

| Parámetro       | Descripción                                                                        |
| --------------- | ---------------------------------------------------------------------------------- |
| **`formula`**   | Fórmula del modelo a ajustar, típicamente `y ~ x`.                                 |
| **`data`**      | Data frame que contiene las variables usadas en la fórmula.                        |
| **`weights`**   | Vector de pesos opcional para ponderar las observaciones.                          |
| **`subset`**    | Vector lógico o de índices para seleccionar un subconjunto de los datos.           |
| **`na.action`** | Función para manejar valores faltantes (por ejemplo, `na.omit`).                   |
| **`model`**     | Si es `TRUE`, incluye el modelo completo en el resultado (útil para predicciones). |

</b>

**Parámetros de control del suavizado**

| Parámetro        | Descripción                                                                                                                  |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **`span`**       | Proporción de datos usados para el suavizado local (default = 0.75). Valores menores producen curvas más sensibles al ruido. |
| **`enp.target`** | Número efectivo de parámetros objetivo; alternativa a `span`.                                                                |
| **`degree`**     | Grado del polinomio local (1 = lineal, 2 = cuadrático). Default = 2.                                                         |

</b>

**Otras opciones de configuración**

| Parámetro         | Descripción                                                                                                                    |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| **`parametric`**  | Vector lógico indicando qué predictores deben tratarse paramétricamente.                                                       |
| **`drop.square`** | Para modelos cuadráticos, si se deben omitir términos cuadrados de ciertos predictores.                                        |
| **`normalize`**   | Si se debe escalar (normalizar) los predictores antes del ajuste (default = `TRUE`).                                           |
| **`family`**      | Familia de funciones de pérdida: `"gaussian"` (default, sin robustez) o `"symmetric"` (usa M-estimadores para mayor robustez). |
| **`method`**      | Método para usar: `"loess"` para ajuste completo, `"model.frame"` solo evalúa la fórmula sin ajuste.                           |
| **`control`**     | Objeto `loess.control()` para ajustes adicionales (como tolerancias, iteraciones, etc.).                                       |
| **`...`**         | Argumentos adicionales pasados a funciones internas.                                                                           |

</b>

## **Pasos generales de los modelos Machine Learning supervisado**

1.  **Importar y explorar los datos**
2.  **Preprocesamiento**
3.  **División de los datos (train/test)**
4.  **Entrenamiento del modelo**
5.  **Evaluación del modelo**
6.  **Ajustes o validación cruzada (si aplica)**
7.  **Predicción con nuevos datos**
8.  **Interpretación de resultados**

<p align="center">
<img src="../../img/ML_Steps.png" alt="Machine Learning Steps" width="100%"/>
</p>

## Base de datos

La base de datos `mtcars` es un conjunto de datos clásico en R que contiene información sobre **32 automóviles** (modelos de 1973–74), y fue extraído de la revista *Motor Trend US*. Incluye **variables técnicas** del desempeño de los autos.

Aquí está una descripción de cada columna:

| Variable | Significado | Tipo de dato |
|-------------------|-----------------------------------|-------------------|
| `mpg` | Miles per gallon (millas por galón) | Numérica |
| `cyl` | Número de cilindros | Entero |
| `disp` | Desplazamiento del motor (en pulgadas cúbicas) | Numérica |
| `hp` | Caballos de fuerza | Entero |
| `drat` | Relación del eje trasero (rear axle ratio) | Numérica |
| `wt` | Peso del auto (en miles de libras) | Numérica |
| `qsec` | Tiempo en 1/4 de milla (segundos) | Numérica |
| `vs` | Tipo de motor: 0 = V-shaped, 1 = straight (en línea) | Binaria (factor) |
| `am` | Tipo de transmisión: 0 = automática, 1 = manual | Binaria (factor) |
| `gear` | Número de velocidades (marchas) adelante | Entero |
| `carb` | Número de carburadores | Entero |

```{r}
# 2. Cargar y se exploran los datos
data("mtcars")
```

```{r, echo = FALSE}
require(gt)

mtcars %>% 
 gt() %>%
  tab_header(title = "mtcars data") %>%
   tab_options(heading.title.font.size = 14, 
                 heading.subtitle.font.size = 12,
                 table.font.names = "Century Gothic",
                 table.font.size = 10,
                 data_row.padding = px(1)) %>%
      tab_style(style = list(cell_text(align = "left",
                                       weight = 'bold')),
                locations = list(cells_title(groups = c("title")))) %>%
       tab_style(style = list(cell_text(align = "left")),
                 locations = list(cells_title(groups = c("subtitle")))) %>%
        cols_width(starts_with("Cuando_") ~ px(300),
                   everything() ~ px(50)) %>%
         as_raw_html() 

```


## Análisis exploratorio de los datos (`EDA`) 

  El **Análisis Exploratorio de Datos (EDA)** es una fase crítica en cualquier proyecto de ciencia de datos, machine learning o estadística. Su propósito principal es **entender los datos** antes de realizar cualquier modelado complejo. No es solo un paso preliminar; es un proceso iterativo de investigación, visualización y resumen que ayuda a descubrir patrones, detectar anomalías y probar hipótesis.


### Comprensión de la Estructura de los Datos
  
   * **Dimensiones:** ¿Cuántas filas (observaciones) y cuántas columnas (variables) hay? Esto te da una idea del tamaño del dataset.
* **Nombres de las Variables:** Identificar cada columna y su significado. ¿Qué representa `mpg`, `hp`, `wt` en el caso de `mtcars`?
  * **Tipos de Datos:** ¿Son numéricas (enteros, decimales), categóricas (factores, cadenas de texto), booleanas? El tipo de dato determina qué análisis y visualizaciones puedes aplicar.
* **Formato y Codificación:** Asegurarte de que los datos estén en un formato utilizable y, si hay texto, que la codificación sea correcta.

### Detección y Manejo de Datos Faltantes

Los datos del mundo real rara vez son perfectos. Los valores faltantes (`NA`, `NaN`, `null`, etc.) son comunes y pueden sesgar tus análisis o hacer que los modelos fallen. Lo fundamental aquí es:
  
  * **Identificar la presencia de datos faltantes:** ¿Hay valores faltantes en alguna variable? ¿Cuántos?
  * **Entender el patrón:** ¿Son aleatorios o hay un patrón en los datos faltantes? (e.g., ¿faltan valores solo en ciertas condiciones o para ciertos grupos?)
* **Decidir una estrategia:** ¿Imputar los valores (reemplazarlos), eliminar las filas/columnas con datos faltantes, o tratarlos de alguna manera especial en el modelado?
  
Utilizando la función `complete.cases(mtcars)` retorna un vector lógico que indica si cada fila **no tiene valores NA** (completamente observada).

```{r}
# Detección si hay alguna fila incompleta
any(!complete.cases(mtcars))
```
Esto indica que **no hay filas incompletas** en `mtcars`.

En el siguiente código sirve para contar el número de valores ausentes (`NA`) en cada columna del conjunto de datos `mtcars`.    

```{r}
require(purrr)

# Número de datos ausentes por variable
map_dbl(mtcars, .f = function(x){sum(is.na(x))})
```

### Análisis de Correlaciones  

```{r, class.source = "fold-hide", fig.show='hold'}
require(corrplot)

corrplot(cor(mtcars),
         type = "upper",
          method = "color",
           order = "hclust",
            tl.col = "blue4",
             tl.offset = 0.1,
              tl.cex = 1,
               tl.srt = 90,
                cl.align.text = "c",
                 number.cex = .5,
                  cl.cex = ,
                   addCoef.col = "white", # Add coefficient of correlation 
                    mar = c(0, 0, 2.5, 0))
```


```{r, class.source = "fold-hide", fig.height=8, fig.width=10}
require(GGally)

ggpairs(mtcars,
         aes(fill = mpg, color = as.factor(cyl), alpha = 0.5),
          columns = 1:10, 
           upper = list(continuous = wrap("cor", size = 2.5)),
            lower = list(continuous = "smooth")) +
             theme_bw() + 
              theme(axis.text = element_text(family = "Montserrat", size = 5)) + 
               scale_color_viridis_d(option = "A", begin = 0.3, end = 0.8) 
```

Se usa la función `findCorrelation()` está diseñada para **identificar y recomendar la eliminación de variables altamente correlacionadas** en un conjunto de datos. La idea es evitar la multicolinealidad, que puede ser un problema en muchos modelos estadísticos y de machine learning (como la regresión lineal), haciendo que los coeficientes sean inestables o difíciles de interpretar.


```{r}
require(caret)
highCorr <- findCorrelation(cor(mtcars), .75)

# Mostrar los nombres de las columnas a remover
if(length(highCorr) > 0) {
  print(names(mtcars)[highCorr])
} else {
  print("No se encontraron variables con alta correlación para el umbral dado.")
}
```

### Varianzas cercanas a cero  

La función `nearZeroVar()` identifica variables que tienen varianza cercana a cero. Estas variables son casi constantes (tienen muy poca o ninguna variabilidad) y, por lo tanto, no aportan mucha información para un modelo predictivo.  


```{r}
library(caret)

# Identificar variables con varianza cercana a cero
# `saveMetrics = TRUE` para obtener un data frame con los detalles
nzv_results <- nearZeroVar(mtcars, saveMetrics = TRUE)
print(nzv_results)
```

### Skewness 

```{r}
require(e1071) 

skewValues = apply(mtcars, 2, skewness)
skewValues
```

### Variable númericas  

```{r, class.source = "fold-hide", fig.height=6, fig.width=8}
require(tidyverse)

p <- mtcars%>%
      gather(key = "Variables", value = "Value") %>%
       ggplot(aes(x = Value, fill = Variables)) +
        geom_bar(alpha = 0.9) +
         geom_density(aes(y = ..count..), stat = "density", alpha = 0.5) +
          theme_minimal() +
           theme(axis.text.x = element_text(size = 10, angle = 90),
                 axis.text.y = element_text(size = 7),
                 legend.position = "none"
                 ) +
             scale_fill_viridis_d(option = "A", begin = 0.3, end = 0.8) +
             labs(y = "",
                  x = "") + 
              facet_wrap(.~Variables, scales = "free") 
p
```

### Transformación Box-Cox  

El paquete `MASS` contiene la función `boxcox()`, que es la base matemática para la **Transformación Box-Cox**. Aunque `caret` tiene su propia implementación para aplicar la transformación, a menudo se basa en funciones de `MASS` para los cálculos subyacentes o para ofrecer una funcionalidad más completa en ciertos contextos de `preProcess`. Es una buena práctica asegurar que esté cargado si se va a usar esta transformación.

<a href="https://dvillasanao.github.io/ML_Examples/Output/R/Stats/Box-Cox.html" style="color: blue;">
  Transformación Box - Cox (Calculo)
</a>

```{r}
require(MASS)
require(caret)

T.BCox <- preProcess(mtcars, method = "BoxCox")

## segPP$bc<-Box-Cox tranformation values 
names(T.BCox$bc) # Muestra los nombres de las variables transformadas


T.BCox$bc$mpg  #Se muestran los detalles específicos de la transformación para la variable "mpg"
```

* La transformación de Box-Cox para `mpg` seleccionó $\lambda = 0$, por lo tanto **se aplicará logaritmo natural a los valores de `mpg`** para hacerlos más simétricos y adecuados para modelos que asumen normalidad.

$$y^{(\lambda=0)} = \log(y)$$
* **Sample Skewness: 0.611**: `mpg` tiene una **asimetría moderada positiva**.
 * **Fudge factor**: Se utiliza a veces para estabilizar la transformación numéricamente cuando $\lambda = 0$, pero no cambia el hecho de que se esté usando logaritmo.


## Entrenamiento de los datos (train/test)

La división de datos en conjuntos de **entrenamiento (train)** y **prueba (test)** es una práctica fundamental en el aprendizaje automático y la modelización predictiva. Su importancia radica en la necesidad de obtener una evaluación **realista y no sesgada** del rendimiento de un modelo, y de asegurar que el modelo sea capaz de **generalizar** a datos nuevos y no vistos.

-   Se usa la función `createDataPartition()` del paquete `caret` para **dividir los datos**.
-   Se crea un **índice** con el 80% de las filas del `mtcars`, **estratificado** según la variable `mpg` (la variable objetivo).
-   El argumento `p = 0.8` significa que el 80% se usará para **entrenamiento** y el 20% restante para **prueba**.
-   `list = FALSE` devuelve los índices como un vector, no como una lista.

```{r}
# Se quiere predecir `mpg` (millas por galón) usando otras variables
# mpg será la variable dependiente (target)

# 3. Se dividen losdatos en entrenamiento y prueba
set.seed(123)  # Para reproducibilidad
train_index <- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)
train_data <- mtcars[train_index, ]
test_data <- mtcars[-train_index, ]
```

-   Particionar los datos, evita el **overfitting** (cuando el modelo memoriza los datos de entrenamiento).
-   Permite una **evaluación honesta** del modelo al probarlo en datos que no vio durante el entrenamiento.
-   Es una práctica estándar en cualquier pipeline de aprendizaje automático.

## Preprocesamiento de los datos  

El **preprocesamiento de datos** es una fase fundamental en análisis de datos y machine learning. La calidad de los datos de entrada determina directamente la calidad de los resultados de los modelos. Es decir, es una **etapa indispensable** que asegura que los datos estén óptimos para el aprendizaje efectivo. Una buena preparación se traduce en **modelos más robustos, precisos y fiables**. 

### Aspectos Clave del Preprocesamiento

1. **Mejora la calidad del modelo**: Siguiendo el principio "Garbage In, Garbage Out", datos limpios permiten que el modelo aprenda patrones más precisos y reducen sesgos.
2. **Optimiza el rendimiento**: Acelera la convergencia de algoritmos iterativos y mejora la eficiencia computacional.
3. **Maneja datos faltantes** mediante imputación o eliminación.
4. **Trata valores atípicos** que pueden distorsionar estadísticas y afectar modelos sensibles.
5. **Normaliza y escala características** para que todas contribuyan equitativamente al modelo.
6. **Codifica variables categóricas** en formato numérico mediante técnicas como One-Hot Encoding.
7. **Reduce la dimensionalidad** para simplificar el modelo y prevenir sobreajuste.
8. **Transforma datos** para mejorar distribuciones y linealizar relaciones.

### Libería `recipes` 

El paquete **`recipes`** (parte del ecosistema **tidymodels**) te permite construir **pipelines de preprocesamiento** de datos **estructurados y reproducibles**. Es muy útil porque:   
  
  * Puedes **encadenar pasos** como normalización, imputación, transformación Box-Cox, codificación, etc.
* El preprocesamiento queda separado del modelo.
* Es muy compatible con flujos modernos de `tidymodels`.  

<a href="https://dvillasanao.github.io/ML_Examples/Output/R/Stats/Recipes_Functions.html" style="color: blue;">
  `Recipes` Function in R
</a>

* **`recipe()`**: Es la función principal para crear un objeto de receta.  
* **`step_normalize()`**: Esta función le indica a la receta que debe **normalizar (estandarizar)** las variables especificadas. La normalización en `recipes` generalmente significa **centrar y escalar** las variables para que tengan una media de 0 y una desviación estándar de 1 (similar a la estandarización Z-score).
* **`step_dummy()`**: Esta función le indica a la receta que debe crear **variables *dummy*** (también conocidas como variables indicadoras o *one-hot encoding*) a partir de las variables categóricas. Para cada variable categórica con $k$ niveles, se crearán $k-1$ nuevas columnas binarias (0 o 1).   
* **`step_nzv()`**: Esta función se utiliza para identificar y potencialmente **eliminar variables con varianza cercana a cero (near-zero variance)**. Estas son variables que tienen muy poca variabilidad o que tienen un solo valor predominante, lo que las hace poco informativas para la mayoría de los modelos.


```{r}
require(recipes)

objeto_recipes <- recipe(mpg ~ ., data = train_data) %>%
                   step_normalize(all_numeric_predictors()) %>%
                    step_corr(all_numeric_predictors(), threshold = 0.7) %>%
                     # step_dummy(all_nominal_predictors()) %>% 
                      step_nzv(all_predictors())
```


## Entrenamiento del modelo

El **entrenamiento de un modelo** es el paso más importante en el aprendizaje automático. Usando un conjunto de datos de entrenamiento (`train_data`), el modelo aprende a reconocer patrones para hacer predicciones confiables con datos nuevos. Este proceso es esencial por estas razones:

-   Durante el entrenamiento, el modelo **identifica patrones específicos** en los datos.
-   Separamos los datos en grupos de entrenamiento y prueba para asegurar que el modelo funcione bien no solo con datos conocidos, sino también con **datos nuevos y no vistos**.
-   Después del entrenamiento con `train_data`, usamos el **conjunto de prueba (`test_data`)** para medir el rendimiento. Las métricas (MAE, RMSE, $R^2$) nos muestran qué tan bien el modelo **maneja datos nuevos**.
-   Un rendimiento peor en `test_data` que en `train_data` indica que el modelo está sobreajustado.
-   Con estos resultados, podemos decidir si el modelo está listo para usar o necesita ajustes.  


Una vez que se ha creado el objeto recipe con todas las transformaciones de preprocesado, se aprenden con los datos de entrenamiento y se aplican a los dos conjuntos. 

- **`prep()`**: Esta es la función clave del paquete `recipes`. Su propósito es **aplicar los pasos de preprocesamiento definidos en tu `objeto_recipes` a tus datos de entrenamiento (`train_data`) y aprender cualquier parámetro necesario**.


```{r}
# Se entrena el objeto recipe
trained_recipe <- prep(objeto_recipes, training = train_data)
```


```{r}
# Se aplican las transformaciones al conjunto de entrenamiento y de test
datos_train_prep <- bake(trained_recipe, new_data = train_data) %>% 
                     as.data.frame()
datos_test_prep  <- bake(trained_recipe, new_data = test_data) %>% 
                     as.data.frame()
```



```{r}
# --- Configurar el Control de Entrenamiento (Resampling) ---
# Usaremos validación cruzada para evaluar el modelo y ajustar los hiperparámetros
ctrl <- trainControl(
                      method = "cv",          # Usar validación cruzada (Cross-Validation)
                      number = 10             # 10 pliegues (folds) para la validación cruzada
                      # summaryFunction = defaultSummary, # Puedes especificar una función para resumir las métricas (por defecto, RMSE y R-squared para regresión)
                      # verboseIter = TRUE    # Muestra el progreso de cada iteración
)
```


```{r}
# Definir un grid de tuning para `span` y `degree`
# Aquí, probaremos dos grados (1 y 2) y varios valores de span
loess_grid <- expand.grid(
                          span = seq(0.1, 1, by = 0.01), # Probar span de 0.5 a 1.0 en incrementos de 0.1
                          degree = c(1)              # Probar grado 1 (lineal) y 2 (cuadrático)
)

```

  
```{r}
require(caret)
require(gam)

# Aplicar el modelo LOESS
modelo_loess <- train(mpg ~ .,
                      data = datos_train_prep,
                      method = "gamLoess",
                      trControl = ctrl,
                      tuneGrid = loess_grid, # Aquí le pasamos nuestro grid de hiperparámetros
                      metric = "RMSE"       # Por defecto, caret para regresión usa RMSE y R-squared
                      )

ggplot(modelo_loess, highlight = TRUE) + 
 theme_bw()
```

```{r}
modelo_loess$bestTune
```


## Referencias

loess function - RDocumentation. (n.d.). Retrieved June 26, 2025, from https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess

W. S. Cleveland, E. Grosse and W. M. Shyu (1992) Local regression models. Chapter 8 of Statistical Models in S eds J.M. Chambers and T.J. Hastie, Wadsworth & Brooks/Cole.

Librerias que se usaron en el documento

```{r, echo = FALSE, eval = TRUE}
sesion_info <- devtools::session_info()
require(knitr)
require(kableExtra)
kable(dplyr::select(tibble::as_tibble(sesion_info$packages %>% dplyr::filter(attached == TRUE)),
                    c(package, loadedversion, source))) %>%
 kable_styling(font_size = 10, 
               bootstrap_options = c("condensed", "responsive", "bordered")) %>%
  kable_classic(full_width = TRUE, html_font = "montserrat") %>% 
   scroll_box(width = "100%", height = "400px") %>%  
    gsub("font-size: initial !important;", "font-size: 10pt !important;", .)
```

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="Creative Commons Licence" style="border-width:0"/></a><br />This work by [**Diana Villasana Ocampo**]{xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"} is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
