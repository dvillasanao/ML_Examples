---
title: "Ordinary Least Squares Regression (OLSR)"
subtitle: "Apuntes y anotaciones personales"
author: "Diana Villasana Ocampo"
format:
  html:
    code-fold: false
    highlight-style: tango
    theme: flatly
    toc: true
    toc-depth: 3
    toc-location: left
engine: jupyter
jupyter: python3
execute:
  echo: true      
  message: false   # Suprimir mensajes generados por R/Python
  warning: false   # Suprimir advertencias generadas por R/Python
  error: false     # Suprimir errores (muestra la ejecuci√≥n, pero no los errores)
  execute-dir: project
  # cache: true    # Habilitar el cach√© (si quieres, desactiva para depuraci√≥n)
output-dir: ../../Output/Regression
---

```{python}
#| include: false
#| echo: false
#| eval: false
import subprocess
import os

input_file = os.path.join(os.getcwd(), "\\R", "\\Regression", "\\01.01.OLSR_py.qmd")
output_dir = os.path.join(os.getcwd(), "\\Output\\Regression\\")

subprocess.run([
    "quarto",
    "render",
    input_file,
    "--output-dir", output_dir
])
```

```{=html}
<style type="text/css">
body {
text-align: justify;
font-style: normal;
font-family: "Montserrat";
font-size: 12px
}
h1.title {
  font-size: 40px;
  color: #000D3B;
}
h1 {
  font-size: 35px;
  color: #B6854D;
}
h2 {
  font-size: 30px;
  color: #172984;
}
h3 {
  font-size: 25px;
  color: #172984;
}
h4 {
  font-size: 22px;
  color: #172984;
}
h5 {
  ont-size: 20px;
  color: #172984;
}
h6{
  ont-size: 18px;
  color: #1864cb;
}
</style>
```

```{=html}
<style>
.nav>li>a {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #1C3BA4
}
.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li>a:focus {
    color: #ffffff;
    background-color: #09C2BC
}
</style>
```

```{=html}
<style>
.tile1-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6A87;
    list-style: none;
}
.top1-tiles a:nth-of-type(1):hover, .top-tiles1 a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6A87
}
</style>
```

```{=html}
<style>
.tile2-text {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #0A6CC8;
    list-style: none;
}
.top2-tiles a:nth-of-type(1):hover, .top2-tiles a:nth-of-type(1):focus{
    color: #ffffff;
    background: #0A6CC8
}
</style>
```

```{=html}
<style>
.math {
  font-size: 15px;
  color: #1e42ab;
}

.callout {
  border: 1px solid red; /* Yellow border */
  background-color: lightgrey; /* Light yellow background */
  padding: 15px;
  margin-bottom: 15px;
  border-left: 5px solid #ffcc00; /* Stronger left border */
}

</style>
```

::: {.callout-note appearance="default" icon="üéØ"}
## Este material es reproducible en c√≥digo Python utilizando Quarto
:::

La Regresi√≥n por M√≠nimos Cuadrados Ordinarios (Ordinary Least Squares Regression, **OLSR** u **OLS**) representa una metodolog√≠a estad√≠stica esencial que permite analizar la correlaci√≥n entre una **variable dependiente** (tambi√©n conocida como variable de respuesta) y una o m√°s **variables independientes** (o predictoras). Este m√©todo constituye una herramienta fundamental en el campo del an√°lisis de regresi√≥n lineal.

<p align="center">
<img src="../../img/Regression/01_image_OLSR.png" alt="Machine Learning Steps" width="40%"/>
</p>

```{python}
#| echo: false
#| eval: false
import sys
print(sys.path)

```


```{python}
#| label: setup-python
#| include: false # Oculta este chunk ya que es solo para configuraci√≥n

# Importar reticulate en Python y asegurar la inicializaci√≥n del puente R
# Esto crea el objeto 'r' en el entorno de Python
#import rpy2.robjects as ro
#import rpy2.situation
#from rpy2.robjects import pandas2ri
#from rpy2.robjects.conversion import localconverter

# Activar la conversi√≥n autom√°tica de R a pandas DataFrame
#pandas2ri.activate()

# Puedes incluso hacer una peque√±a prueba para asegurarte de que R est√° activo
# print(ro.r('R.version.string'))
```

**Librer√≠as que se usaron en el documento**

```{python}
#| label: load-py-pckgs
import re
import rpy2
import pandas as pd
from pathlib import Path
import seaborn as sns
import os # Necesario para la funci√≥n os.makedirs
```


```{python}
#| echo: false
#| eval: true

# Datos de la tabla
criterios = [
    "üîç Tipo de modelo",
    "üéØ Variable respuesta",
    "üî¢ Variables predictoras",
    "üìà Relaci√≥n entre variables",
    "üß™ Normalidad de residuos",
    "üîÅ Independencia de errores",
    "‚öñÔ∏è Homoscedasticidad",
    "‚ùó Sensible a outliers",
    "üîó Multicolinealidad entre predictores",
    "üß† Interpretabilidad",
    "üöÄ Velocidad y eficiencia",
    "üß™ Validaci√≥n cruzada",
    "‚ùå No funciona bien si..."
]

aplica = [
    "Supervisado",
    "Num√©rica continua",
    "Num√©ricas y/o categ√≥ricas",
    "Lineal (supuesto clave)",
    "Deseable",
    "Necesaria",
    "Necesaria",
    "S√≠",
    "Problema com√∫n",
    "Alta",
    "Muy alta",
    "Compatible",
    "Relaciones no lineales, outliers severos, colinealidad"
]

detalles = [
    "Se entrena con datos X ‚Üí y",
    "Ej. mpg, precio, ingresos",
    "Categor√≠as convertidas a dummies",
    "Se asume una relaci√≥n lineal entre X e Y",
    "Importante para intervalos de confianza v√°lidos",
    "Errores deben ser independientes",
    "Varianza de errores debe ser constante",
    "Outliers pueden influir mucho en el modelo",
    "Usar VIF para detectar problemas",
    "Modelo f√°cil de explicar",
    "R√°pido incluso con datos grandes",
    "Ayuda a prevenir overfitting",
    "Evitar si no hay linealidad o hay muchos outliers"
]

# Crear DataFrame de Pandas
tabla_olsr = pd.DataFrame({
    "Criterio": criterios,
    "Aplica": aplica,
    "Detalles": detalles
})

# Estilizar la tabla con Pandas Styler
# Nota: La estilizaci√≥n exacta como 'gt' es compleja de replicar pixel a pixel con Pandas Styler
# pero podemos acercarnos a la mayor√≠a de los requerimientos.
# Para fuentes personalizadas como 'Century Gothic', puede que necesites CSS externo
# o que el navegador del usuario tenga la fuente instalada.

styled_table = (
    tabla_olsr.style
    .set_table_attributes("style='font-family: Century Gothic; font-size: 10pt;'")
    .set_caption("<h2 style='text-align: left; font-weight: bold; font-size: 14pt;'>Gu√≠a r√°pida para elegir OLSR</h2><p style='text-align: left; font-size: 12pt;'>Fuente: Elaboraci√≥n propia</p>")
    .set_properties(subset=['Criterio', 'Aplica'], **{'width': '200px'})
    .set_properties(subset=['Detalles'], **{'width': '500px'})
    .set_properties(**{'text-align': 'left'}) # Alinea todo a la izquierda por defecto
    .set_table_styles([ # Para aplicar el padding a las celdas
        {'selector': 'td', 'props': [('padding', '1px')]}
    ], overwrite=False)
)

styled_table
```



## Objetivo

La Regresi√≥n por M√≠nimos Cuadrados Ordinarios (`OLSR`) busca la l√≠nea que mejor se ajusta a los datos. Para lograrlo, reduce al m√≠nimo la suma de los cuadrados de las diferencias entre los valores reales y los valores que predice el modelo. Estas diferencias son los **residuos** o **errores**. Al trabajar con los cuadrados de los errores, este m√©todo evita que los errores positivos y negativos se anulen entre s√≠, y da m√°s peso a los errores grandes durante el proceso de minimizaci√≥n.

## Metodolog√≠a

La metodolog√≠a de OLSR se basa en los siguientes pasos y principios:

1.  **Modelo Lineal:** OLSR asume una relaci√≥n lineal entre las variables. Para una regresi√≥n lineal simple (una variable independiente), la ecuaci√≥n es:\
    $$Y = \beta_0 + \beta_1X + \epsilon$$

    Donde:

    -   $Y$ es la variable dependiente.

-   $X$ es la variable independiente.
-   $\beta_0$ es el intercepto (el valor de $Y$ cuando $X$ es 0).
-   $\beta_1$ es la pendiente (el cambio en $Y$ por cada unidad de cambio en $X$).
-   $\epsilon$ es el t√©rmino de error o residual, que representa la parte de $Y$ que no puede ser explicada por $X$.

Para una regresi√≥n lineal m√∫ltiple (varias variables independientes), la ecuaci√≥n se expande a:\
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$$

2.  **Minimizaci√≥n de la Suma de Cuadrados de Residuos (SSR):** El coraz√≥n de OLS es encontrar los valores de los coeficientes ($\beta_0, \beta_1$, etc.) que minimicen la siguiente funci√≥n:\
    $$\text{Minimizar } \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$\
    Donde:
    -   $y_i$ es el valor observado de la variable dependiente para la observaci√≥n $i$.

-   $\hat{y}_i$ es el valor predicho de la variable dependiente por el modelo para la observaci√≥n $i$.
-   $(y_i - \hat{y}_i)$ es el residual para la observaci√≥n $i$.

Para lograr esta minimizaci√≥n, se utilizan t√©cnicas de c√°lculo (derivadas parciales) para encontrar los valores de los coeficientes que hacen que la pendiente de la funci√≥n de suma de cuadrados sea cero.

3.  **Estimaci√≥n de Coeficientes:** Los valores estimados de los coeficientes, denotados como $\hat{\beta}_0, \hat{\beta}_1$, etc., son aquellos que resultan de la minimizaci√≥n. Estos coeficientes son los que definen la "l√≠nea de mejor ajuste".

4.  **Supuestos del OLS:** Para que los estimadores de OLS sean los "mejores estimadores lineales insesgados" (seg√∫n el Teorema de Gauss-Markov), se deben cumplir ciertas suposiciones:

    -   **Linealidad:** La relaci√≥n entre las variables es lineal.

    -   **Independencia de los errores:** Los errores de una observaci√≥n no est√°n correlacionados con los errores de otra.

    -   **Homocedasticidad:** La varianza de los errores es constante en todos los niveles de las variables independientes.

    -   **Normalidad de los errores:** Los errores se distribuyen normalmente (aunque no es estrictamente necesario para la estimaci√≥n, s√≠ lo es para la inferencia estad√≠stica).

    -   **No multicolinealidad perfecta:** Las variables independientes no est√°n perfectamente correlacionadas entre s√≠.

## **Pasos generales del Machine Learning supervisado**

1.  **Importar y explorar los datos**
2.  **Preprocesamiento**
3.  **Divisi√≥n de los datos (train/test)**
4.  **Entrenamiento del modelo**
5.  **Evaluaci√≥n del modelo**
6.  **Ajustes o validaci√≥n cruzada (si aplica)**
7.  **Predicci√≥n con nuevos datos**
8.  **Interpretaci√≥n de resultados**

<p align="center">
<img src="../../img/ML_Steps.png" alt="Machine Learning Steps" width="100%"/>
</p>

------------------------------------------------------------------------

## Base de datos

La base de datos `mtcars` es un conjunto de datos cl√°sico en R que contiene informaci√≥n sobre **32 autom√≥viles** (modelos de 1973‚Äì74), y fue extra√≠do de la revista *Motor Trend US*. Incluye **variables t√©cnicas** del desempe√±o de los autos.

Aqu√≠ est√° una descripci√≥n de cada columna:

| Variable | Significado | Tipo de dato |
|-------------------|-----------------------------------|-------------------|
| `mpg` | Miles per gallon (millas por gal√≥n) | Num√©rica |
| `cyl` | N√∫mero de cilindros | Entero |
| `disp` | Desplazamiento del motor (en pulgadas c√∫bicas) | Num√©rica |
| `hp` | Caballos de fuerza | Entero |
| `drat` | Relaci√≥n del eje trasero (rear axle ratio) | Num√©rica |
| `wt` | Peso del auto (en miles de libras) | Num√©rica |
| `qsec` | Tiempo en 1/4 de milla (segundos) | Num√©rica |
| `vs` | Tipo de motor: 0 = V-shaped, 1 = straight (en l√≠nea) | Binaria (factor) |
| `am` | Tipo de transmisi√≥n: 0 = autom√°tica, 1 = manual | Binaria (factor) |
| `gear` | N√∫mero de velocidades (marchas) adelante | Entero |
| `carb` | N√∫mero de carburadores | Entero |





```{python}
#| eval: false
require(reticulate)
reticulate::repl_python() #can be used to interactively run Python code
# 2. Cargar y se exploran los datos
data("mtcars")
```


```{python}
#| echo: true
#| eval: false 

#pip install openpyxl 
import pandas as pd
from pathlib import Path
import os # Necesario para la funci√≥n os.makedirs

# Cargar la base de datos mtcars directamente desde el entorno de R
# r.mtcars accede al objeto 'mtcars' que R ha puesto a disposici√≥n
# Reticulate autom√°ticamente lo convierte a un DataFrame de Pandas.
mtcars_df = r.mtcars

## Se guarda la base de datos en un archivo Excel  
file = Path.cwd().parent.parent / "Data"

mtcars_df.to_excel(file /"mtcars_data.xlsx", index=True)
```


```{python}
mtcars_df = pd.read_excel(Path.cwd().parent.parent / "Data" / "mtcars_data.xlsx")
```


```{python}
#| echo: false
mtcars_df.head(10)
```


## Entrenamiento de los datos (train/test)

La divisi√≥n de datos en conjuntos de **entrenamiento (train)** y **prueba (test)** es una pr√°ctica fundamental en el aprendizaje autom√°tico y la modelizaci√≥n predictiva. Su importancia radica en la necesidad de obtener una evaluaci√≥n **realista y no sesgada** del rendimiento de un modelo, y de asegurar que el modelo sea capaz de **generalizar** a datos nuevos y no vistos.

* En Python, `train_test_split()` devuelve directamente los cuatro conjuntos de datos:
  * `X_train`: Predictoras para entrenamiento.
* `X_test`: Predictoras para prueba.
* `y_train`: Variable objetivo para entrenamiento.
* `y_test`: Variable objetivo para prueba.

```{python}
#| label: split-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np # Necesario si necesitas setear una semilla para numpy/pandas

# Definir la variable objetivo (dependiente)
target_variable = 'mpg'

# X contendr√° todas las variables predictoras (features)
# Y contendr√° la variable objetivo
X = mtcars_df.drop(columns=[target_variable])
y = mtcars_df[target_variable]

# 3. Dividir los datos en entrenamiento y prueba
# random_state es el equivalente a set.seed() para reproducibilidad
# test_size=0.2 significa que el 20% de los datos ir√°n al conjunto de prueba (80% para entrenamiento)
# Esto es importante para asegurar que las proporciones de 'mpg' sean similares en ambos conjuntos,
# especialmente √∫til para variables categ√≥ricas o si la distribuci√≥n de 'mpg' es importante.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)


print(f"Tama√±o del conjunto de entrenamiento (X_train): {X_train.shape}")
print(f"Tama√±o del conjunto de prueba (X_test): {X_test.shape}")
print(f"Tama√±o del conjunto de entrenamiento (y_train): {y_train.shape}")
print(f"Tama√±o del conjunto de prueba (y_test): {y_test.shape}")
```


-   Particionar los datos, evita el **overfitting** (cuando el modelo memoriza los datos de entrenamiento).
-   Permite una **evaluaci√≥n honesta** del modelo al probarlo en datos que no vio durante el entrenamiento.
-   Es una pr√°ctica est√°ndar en cualquier pipeline de aprendizaje autom√°tico.

## Entrenamiento del modelo

El **entrenamiento de un modelo** es el paso m√°s importante en el aprendizaje autom√°tico. Usando un conjunto de datos de entrenamiento (`train_data`), el modelo aprende a reconocer patrones para hacer predicciones confiables con datos nuevos. Este proceso es esencial por estas razones:

-   Durante el entrenamiento, el modelo **identifica patrones espec√≠ficos** en los datos.
-   Separamos los datos en grupos de entrenamiento y prueba para asegurar que el modelo funcione bien no solo con datos conocidos, sino tambi√©n con **datos nuevos y no vistos**.
-   Despu√©s del entrenamiento con `train_data`, usamos el **conjunto de prueba (`test_data`)** para medir el rendimiento. Las m√©tricas (MAE, RMSE, $R^2$) nos muestran qu√© tan bien el modelo **maneja datos nuevos**.
-   Un rendimiento peor en `test_data` que en `train_data` indica que el modelo est√° sobreajustado.
-   Con estos resultados, podemos decidir si el modelo est√° listo para usar o necesita ajustes.

**Funci√≥n de entrenamiento**

  * `modelo_ols_py = smf.ols(formula=formula, data=train_data_combined).fit()`:
  * `smf.ols()`: Es la funci√≥n para ajustar un modelo de M√≠nimos Cuadrados Ordinarios (Ordinary Least Squares - OLS).
* `formula=formula`: Le pasamos la cadena de f√≥rmula que definimos.
* `data=train_data_combined`: Le indicamos de qu√© DataFrame debe tomar las variables.
* `.fit()`: Este m√©todo entrena el modelo sobre los datos.

    
```{python}
#| label: train-ols-model
#| echo: true
#| message: false
#| warning: false
#| results: as-is # Importante para mostrar el resumen del modelo como HTML

import pandas as pd
import statsmodels.formula.api as smf # Para la sintaxis tipo R de f√≥rmulas

# Asumiendo que X_train, X_test, y_train, y_test ya est√°n definidos
# por el chunk de divisi√≥n de datos anterior.

# Para asegurar que X_train y y_train se combinen correctamente para statsmodels
# statsmodels prefiere un solo DataFrame que contenga todas las variables
# Esto es similar a c√≥mo `lm` en R usa `data = train_data`

# Combinar X_train y y_train en un solo DataFrame para statsmodels
train_data_combined = pd.concat([X_train, y_train], axis=1)

# Definir la f√≥rmula del modelo (similar a R)
# mpg ~ . significa "mpg en funci√≥n de todas las dem√°s variables en el DataFrame"
# Para statsmodels, es mejor listar expl√≠citamente las columnas si X_train tiene muchas
# O si quieres un subconjunto espec√≠fico, lo especificas aqu√≠.
# Aqu√≠ construimos la f√≥rmula din√°micamente para incluir todas las variables de X_train
formula = f"{y_train.name} ~ " + " + ".join(X_train.columns)

# 4. Entrenar el modelo de regresi√≥n lineal (OLS)
# statsmodels.formula.api permite usar la sintaxis de f√≥rmula similar a R
modelo_ols_py = smf.ols(formula=formula, data=train_data_combined).fit()

# Revisar resumen del modelo (similar a summary() en R)
print("Resumen del Modelo OLS en Python:")
print(modelo_ols_py.summary()) # .as_html() para una salida bonita en Quarto
```
    